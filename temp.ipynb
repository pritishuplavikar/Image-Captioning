{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "token = './Flickr8k_text/Flickr8k.token.txt'\n",
    "captions = open(token, 'r').read().strip().split('\\n')\n",
    "\n",
    "caption_data = {}\n",
    "for i, row in enumerate(captions):\n",
    "    row = row.split('\\t')\n",
    "    row[0] = row[0][:len(row[0])-2]\n",
    "    if row[0] in caption_data:\n",
    "        caption_data[row[0]].append(row[1])\n",
    "    else:\n",
    "        caption_data[row[0]] = [row[1]]\n",
    "\n",
    "image_path = './Flickr8k_Dataset/Flickr8k_Dataset/'\n",
    "images = glob.glob(image_path+'*.jpg')\n",
    "\n",
    "def split_data(list_images, images):\n",
    "    temp_train_images = []\n",
    "    for image in images:\n",
    "        if image[len(image_path):] in list_images:\n",
    "            temp_train_images.append(image)\n",
    "\n",
    "    return temp_train_images\n",
    "\n",
    "train_images_file = './Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train_images = set(open(train_images_file, 'r').read().strip().split('\\n'))\n",
    "train_images = split_data(train_images, images)\n",
    "\n",
    "dev_images_file = 'Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "dev_images = set(open(dev_images_file, 'r').read().strip().split('\\n'))\n",
    "dev_images = split_data(dev_images, images)\n",
    "\n",
    "train_images = train_images + dev_images\n",
    "\n",
    "test_images_file = './Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "test_images = set(open(test_images_file, 'r').read().strip().split('\\n'))\n",
    "test_images = split_data(test_images, images)\n",
    "\n",
    "def preprocess_input(x):\n",
    "    x /= 255.\n",
    "    x -= 0.5\n",
    "    x *= 2.\n",
    "\n",
    "    return x\n",
    "\n",
    "def preprocess(image_path):\n",
    "    image = image.load_img(image_path, target_size=(299, 299))\n",
    "    x = image.img_to_array(image)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        \n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "inception_v3 = torchvision.models.inception_v3(pretrained=True)\n",
    "inception_v3.fc = Flatten()\n",
    "\n",
    "inception_v3 = inception_v3.cuda()\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "# cudnn.benchmark = True\n",
    "inception_v3.eval()\n",
    "\n",
    "def encode(image_path, data_transform, inception_v3):\n",
    "    image = Image.open(image_path)\n",
    "    image = data_transform(image).cuda()\n",
    "    image_variable = Variable(torch.unsqueeze(image, 0), volatile=True)\n",
    "    output = inception_v3.forward(image_variable)\n",
    "    encoded_image = torch.squeeze(output, 0)\n",
    "\n",
    "    return encoded_image\n",
    "\n",
    "# encoding_train = {}\n",
    "# for image in tqdm(train_images):\n",
    "#     inception_v3 = inception_v3.cuda()\n",
    "#     encoding_train[image[len(image_path):]] = encode(image, data_transform, inception_v3)\n",
    "    \n",
    "# with open(\"encoded_images_inceptionV3.p\", \"wb\") as encoded_pickle:\n",
    "#     pickle.dump(encoding_train, encoded_pickle)\n",
    "\n",
    "encoding_train = pickle.load(open('encoded_images_inceptionV3.p', 'rb'))\n",
    "\n",
    "# encoding_test = {}\n",
    "# for image in tqdm(test_images):\n",
    "#     encoding_test[image[len(image_path):]] = encode(image, data_transform, inception_v3)\n",
    "    \n",
    "# with open(\"encoded_images_test_inceptionV3.p\", \"wb\") as encoded_pickle:\n",
    "#     pickle.dump(encoding_test, encoded_pickle)\n",
    "    \n",
    "encoding_test = pickle.load(open('encoded_images_test_inceptionV3.p', 'rb'))\n",
    "\n",
    "inception_v3 = None\n",
    "\n",
    "train_data = {}\n",
    "for image in train_images:\n",
    "    if image[len(image_path):] in caption_data:\n",
    "        train_data[image] = caption_data[image[len(image_path):]]\n",
    "\n",
    "test_data = {}\n",
    "for image in test_images:\n",
    "    if image[len(image_path):] in caption_data:\n",
    "        test_data[image] = caption_data[image[len(image_path):]]\n",
    "\n",
    "all_captions = []\n",
    "for image, captions in train_data.items():\n",
    "    for caption in captions:\n",
    "        all_captions.append('<start> ' + caption + ' <end>')\n",
    "\n",
    "words = [caption.split() for caption in all_captions]\n",
    "\n",
    "# unique_words = []\n",
    "# for word in words:\n",
    "#     unique_words.extend(word)\n",
    "    \n",
    "# unique_words = list(set(unique_words))\n",
    "\n",
    "# with open(\"unique_words.p\", \"wb\") as pickle_d:\n",
    "#     pickle.dump(unique_words, pickle_d)\n",
    "\n",
    "unique_words = pickle.load(open('unique_words.p', 'rb'))\n",
    "vocab_size = len(unique_words)\n",
    "\n",
    "word2idx = {value:index for index, value in enumerate(unique_words)}\n",
    "idx2word = {index:value for index, value in enumerate(unique_words)}\n",
    "\n",
    "max_len = 0\n",
    "for caption in all_captions:\n",
    "    caption_words = caption.split()\n",
    "    if len(caption_words) > max_len:\n",
    "        max_len = len(caption_words)\n",
    "\n",
    "# f = open('flickr8k_training_dataset.txt', 'w')\n",
    "# f.write(\"image_id\\tcaptions\\n\")\n",
    "\n",
    "# for image, captions in train_data.items():\n",
    "#     for caption in captions:\n",
    "#         f.write(image[len(image_path):] + \"\\t\" + \"<start> \" + caption +\" <end>\" + \"\\n\")\n",
    "\n",
    "# f.close()\n",
    "\n",
    "data_file = pd.read_csv('flickr8k_training_dataset.txt', delimiter='\\t')\n",
    "caps = [caption for caption in data_file['captions']]\n",
    "imgs = [image for image in data_file['image_id']]\n",
    "\n",
    "samples_per_epoch = 0\n",
    "for caption in caps:\n",
    "    samples_per_epoch += len(caption.split()) - 1\n",
    "\n",
    "def pad_sequences(sequences, maxlen):\n",
    "    padded_output = []\n",
    "    for index, sequence in enumerate(sequences):\n",
    "        diff = maxlen - len(sequence)\n",
    "        padded_output.append(sequence + [0] * diff)\n",
    "\n",
    "    return padded_output\n",
    "\n",
    "# Needs 16.3 GB storage space\n",
    "\n",
    "# data_file = data_file.sample(frac=1)\n",
    "# iter = data_file.iterrows()\n",
    "# caps = []\n",
    "# imgs = []\n",
    "# for _ in range(data_file.shape[0]):\n",
    "#     x = next(iter)\n",
    "#     caps.append(x[1][1])\n",
    "#     imgs.append(x[1][0])\n",
    "\n",
    "# partial_caps = []\n",
    "# next_words = []\n",
    "# images = []\n",
    "# samples = []\n",
    "# for k in range (10):\n",
    "#     for j, text in enumerate(tqdm(caps[k*3500:(k+1)*3500])):\n",
    "#         current_image = encoding_train[imgs[j]]\n",
    "#         for i in range(len(text.split())-1):\n",
    "#             partial = [word2idx[txt] for txt in text.split()[:i+1]]\n",
    "#             partial_caps.append(partial)\n",
    "\n",
    "#             n = np.zeros(vocab_size)\n",
    "#             n[word2idx[text.split()[i+1]]] = 1\n",
    "#             next_words.append(Variable(torch.FloatTensor(n)))\n",
    "\n",
    "#             images.append(current_image)\n",
    "\n",
    "#     partial_caps = pad_sequences(partial_caps, max_len)\n",
    "#     partial_caps = Variable(torch.FloatTensor(partial_caps))\n",
    "\n",
    "#     torch.save(images, \"./dataset/data_\"+str(k+1)+\"_images\")\n",
    "#     torch.save(partial_caps, \"./dataset/data_\"+str(k+1)+\"_partial_caps\")\n",
    "#     torch.save(next_words, \"./dataset/data_\"+str(k+1)+\"_next_words\")\n",
    "\n",
    "#     partial_caps = []\n",
    "#     next_words = []\n",
    "#     images = []\n",
    "\n",
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module, batch_first=False):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        # Squash samples and timesteps into a single axis\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  # (samples * timesteps, input_size)\n",
    "\n",
    "        y = self.module(x_reshape)\n",
    "\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-1))  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "\n",
    "        return y\n",
    "\n",
    "image_embedding_size = 300\n",
    "word_embedding_size = 300\n",
    "lstm1_units = 256\n",
    "lstm2_units = 300\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "def train(data_path, model):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = optim.SGD(final_model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    training_loss = 0.0\n",
    "\n",
    "    for epoch in range(1):\n",
    "        for data_part in range(1, 2):\n",
    "            images = torch.load(data_path+\"data_\"+str(data_part)+\"_images\")\n",
    "            partial_caps = torch.load(data_path+\"data_\"+str(data_part)+\"_partial_caps\")\n",
    "            next_words = torch.load(data_path+\"data_\"+str(data_part)+\"_next_words\")\n",
    "\n",
    "            for index in tqdm(range(5)): # max(len(images), len(partial_caps), len(next_words))\n",
    "                image_vector = images[index].cuda()\n",
    "                caption_vector = partial_caps[index]\n",
    "                next_word = next_words[index].cuda()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                predicted_word = model(image_vector.unsqueeze(0), caption_vector.unsqueeze(0))\n",
    "                loss = criterion(predicted_word, next_word)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                training_loss += loss.data[0]\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(epoch + 1, \":\", \"Loss:\", training_loss)\n",
    "\n",
    "        training_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ImageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageModel, self).__init__()\n",
    "        self.fc = nn.Linear(2048, image_embedding_size).cuda()\n",
    "        self.relu = nn.ReLU().cuda()\n",
    "\n",
    "    def forward(self, input):\n",
    "        temp = self.relu(self.fc(input))\n",
    "        print (temp.size())\n",
    "        temp = torch.squeeze(temp, 0)\n",
    "        return temp.repeat(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CaptionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CaptionModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, word_embedding_size).cuda()\n",
    "        self.lstm = nn.LSTM(word_embedding_size, lstm1_units, 1, batch_first=True).cuda()\n",
    "        self.td = TimeDistributed(nn.Linear(lstm1_units, word_embedding_size).cuda(), batch_first=True).cuda()\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded_output = self.embedding(torch.LongTensor(input.data).cuda()).cuda()\n",
    "        output, hidden = self.lstm(embedded_output)\n",
    "        return self.td(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FinalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FinalModel, self).__init__()\n",
    "        self.image_model = ImageModel().cuda()\n",
    "        self.caption_model = CaptionModel().cuda()\n",
    "        input_size = image_embedding_size + word_embedding_size\n",
    "        self.lstm = nn.LSTM(input_size, lstm2_units, 1, bidirectional=True, batch_first=True).cuda()\n",
    "        self.fc = nn.Linear(batch_size*max_len*2*lstm2_units, vocab_size).cuda()\n",
    "        self.softmax = nn.Softmax().cuda()\n",
    "\n",
    "    def forward(self, image_vector, caption_vector):\n",
    "        merged_input = torch.cat((self.image_model(image_vector), self.caption_model(caption_vector)), 1).cuda()\n",
    "        output, hidden = self.lstm(merged_input.view(-1))\n",
    "        return self.softmax(self.fc(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "final_model = FinalModel().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 300])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "torch.LongTensor constructor received an invalid combination of arguments - got (torch.FloatTensor), but expected one of:\n * no arguments\n * (int ...)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n * (torch.LongTensor viewed_tensor)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n * (torch.Size size)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n * (torch.LongStorage data)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n * (Sequence data)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5ce389164cb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./dataset/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-1954d7a0efd4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data_path, model)\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m                 \u001b[0mpredicted_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-47faebb085d3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image_vector, caption_vector)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mmerged_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaption_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-e71a5b57f40a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0membedded_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: torch.LongTensor constructor received an invalid combination of arguments - got (torch.FloatTensor), but expected one of:\n * no arguments\n * (int ...)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n * (torch.LongTensor viewed_tensor)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n * (torch.Size size)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n * (torch.LongStorage data)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n * (Sequence data)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "final_model = train(\"./dataset/\", final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def count_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614700 3322992 434031407\n"
     ]
    }
   ],
   "source": [
    "# im = ImageModel()\n",
    "# cm = CaptionModel()\n",
    "# print (count_parameters(im), count_parameters(cm), count_parameters(final_model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
